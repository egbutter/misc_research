{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Union{Nothing, VersionNumber}} with 3 entries:\n",
       "  \"HDF5\"   => v\"0.12.5\"\n",
       "  \"IJulia\" => v\"1.20.2\"\n",
       "  \"Flux\"   => v\"0.10.0\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pkg.installed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test 1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "println(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h': ASCII/Unicode U+0068 (category Ll: Letter, lowercase)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1[1]    # Julia is 1-indexed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Jonathan\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jonathan.\n"
     ]
    }
   ],
   "source": [
    "println(\"Hello $name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello Jonathan\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 * \" \" * name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching Dict(::getfield(Main, Symbol(\"##8#10\")), ::getfield(Main, Symbol(\"##9#11\")))\nClosest candidates are:\n  Dict(::Any) at dict.jl:127",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching Dict(::getfield(Main, Symbol(\"##8#10\")), ::getfield(Main, Symbol(\"##9#11\")))\nClosest candidates are:\n  Dict(::Any) at dict.jl:127",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[38]:1"
     ]
    }
   ],
   "source": [
    "my_phonebook = Dict(Jenny->\"867-5309\", Ghostbusters->\"2222\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Printf\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "changeNum (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function changeNum()\n",
    "    x::Int8 = 10\n",
    "    x = \"Dog\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Array{Int32,2}:\n",
       " 0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = zeros(Int32, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for pid in workers()\n",
    "    println(pid)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.2.0\n",
      "Commit c6da87ff4b (2019-08-20 00:03 UTC)\n",
      "Platform Info:\n",
      "  OS: Linux (x86_64-pc-linux-gnu)\n",
      "  CPU: Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-6.0.1 (ORCJIT, haswell)\n",
      "Environment:\n",
      "  JULIA_DEPOT_PATH = /opt/julia\n",
      "  JULIA_VERSION = 1.2.0\n",
      "  JULIA_PKGDIR = /opt/julia\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JULIA_CUDA_SILENT = true   # Turn off notifications about GPU/CUDA; doesn't seem to work (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `/opt/julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %]  42.1 % [==================================>      ]  84.6 %\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m FixedPointNumbers ─ v0.6.1\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m ProgressMeter ───── v1.2.0\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `/opt/julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [92933f4c]\u001b[39m\u001b[92m + ProgressMeter v1.2.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `/opt/julia/environments/v1.2/Manifest.toml`\n",
      " \u001b[90m [53c48c17]\u001b[39m\u001b[95m ↓ FixedPointNumbers v0.7.0 ⇒ v0.6.1\u001b[39m\n",
      " \u001b[90m [92933f4c]\u001b[39m\u001b[92m + ProgressMeter v1.2.0\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"ProgressMeter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling ProgressMeter [92933f4c-e287-5a05-a399-4b506db050ca]\n",
      "└ @ Base loading.jl:1242\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: crossentropy, normalise, onecold, onehotbatch\n",
    "using ProgressMeter\n",
    "using Random\n",
    "using Statistics: mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Union{Nothing, VersionNumber}} with 4 entries:\n",
       "  \"HDF5\"          => v\"0.12.5\"\n",
       "  \"IJulia\"        => v\"1.20.2\"\n",
       "  \"Flux\"          => v\"0.10.0\"\n",
       "  \"ProgressMeter\" => v\"1.2.0\""
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkgs = Pkg.installed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Flux.Data.Iris.labels()\n",
    "features = Flux.Data.Iris.features();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\", \"Iris-setosa\"], [5.1 4.9 … 4.6 5.0; 3.5 3.0 … 3.1 3.6; 1.4 1.4 … 1.5 1.4; 0.2 0.2 … 0.2 0.2])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note features is features x samples (opposite of usual)\n",
    "\n",
    "labels[1:5], features[:, 1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×150 Array{Float64,2}:\n",
       " -0.900681  -1.14302   -1.38535   …   0.795669  0.432165   0.0686618\n",
       "  1.03206   -0.124958   0.337848     -0.124958  0.800654  -0.124958 \n",
       " -1.34127   -1.34127   -1.39814       0.819624  0.933356   0.762759 \n",
       " -1.31298   -1.31298   -1.31298       1.05354   1.44796    0.790591 "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subract mean, divide by std dev for normed mean of 0 and std dev of 1.\n",
    "normed_features = normalise(features, dims=2)   # 2 means rowwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "onehotbatch(ls, labels[, unk...])\n",
       "\\end{verbatim}\n",
       "Create an \\href{@ref}{\\texttt{OneHotMatrix}} with a batch of labels based on possible \\texttt{labels} set, returns the \\texttt{onehot(unk, labels)} if given labels \\texttt{ls} is not found in set \\texttt{labels}.\n",
       "\n",
       "\\subsection{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> using Flux: onehotbatch\n",
       "\n",
       "julia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n",
       "3×3 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 0  1  0\n",
       " 1  0  1\n",
       " 0  0  0\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "onehotbatch(ls, labels[, unk...])\n",
       "```\n",
       "\n",
       "Create an [`OneHotMatrix`](@ref) with a batch of labels based on possible `labels` set, returns the `onehot(unk, labels)` if given labels `ls` is not found in set `labels`.\n",
       "\n",
       "## Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> using Flux: onehotbatch\n",
       "\n",
       "julia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n",
       "3×3 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 0  1  0\n",
       " 1  0  1\n",
       " 0  0  0\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  onehotbatch(ls, labels[, unk...])\u001b[39m\n",
       "\n",
       "  Create an \u001b[36mOneHotMatrix\u001b[39m with a batch of labels based on possible \u001b[36mlabels\u001b[39m set,\n",
       "  returns the \u001b[36monehot(unk, labels)\u001b[39m if given labels \u001b[36mls\u001b[39m is not found in set\n",
       "  \u001b[36mlabels\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ==========\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> using Flux: onehotbatch\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> onehotbatch([:b, :a, :b], [:a, :b, :c])\u001b[39m\n",
       "\u001b[36m  3×3 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\u001b[39m\n",
       "\u001b[36m   0  1  0\u001b[39m\n",
       "\u001b[36m   1  0  1\u001b[39m\n",
       "\u001b[36m   0  0  0\u001b[39m"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?onehotbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×150 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     1  1  1  1  1  1  1  1  1  1  1  1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klasses = sort(unique(labels))\n",
    "onehot_labels = onehotbatch(labels, klasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"Iris-setosa\"    \n",
       " \"Iris-versicolor\"\n",
       " \"Iris-virginica\" "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×150 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     1  1  1  1  1  1  1  1  1  1  1  1"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets, 2/3 for training, 1/3 for test.\n",
    "# start:step:end and the ; means to concatenate the two\n",
    "# train_indices = [1:3:150 ; 2:3:150];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is more like the sklearn way\n",
    "idxs = randperm(length(labels))\n",
    "train_cut = Int(length(labels) * 0.66)\n",
    "train_indices = idxs[1:train_cut]\n",
    "test_indices = idxs[train_cut:length(labels)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normed_features[:, train_indices]\n",
    "y_train = onehot_labels[:, train_indices]\n",
    "\n",
    "X_test = normed_features[:, test_indices]\n",
    "y_test = onehot_labels[:, test_indices];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mh\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Chain(layers...)\n",
       "\\end{verbatim}\n",
       "Chain multiple layers / functions together, so that they are called in sequence on a given input.\n",
       "\n",
       "\\begin{verbatim}\n",
       "m = Chain(x -> x^2, x -> x+1)\n",
       "m(5) == 26\n",
       "\n",
       "m = Chain(Dense(10, 5), Dense(5, 2))\n",
       "x = rand(10)\n",
       "m(x) == m[2](m[1](x))\n",
       "\\end{verbatim}\n",
       "\\texttt{Chain} also supports indexing and slicing, e.g. \\texttt{m[2]} or \\texttt{m[1:end-1]}. \\texttt{m[1:3](x)} will calculate the output of the first three layers.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "Chain(layers...)\n",
       "```\n",
       "\n",
       "Chain multiple layers / functions together, so that they are called in sequence on a given input.\n",
       "\n",
       "```julia\n",
       "m = Chain(x -> x^2, x -> x+1)\n",
       "m(5) == 26\n",
       "\n",
       "m = Chain(Dense(10, 5), Dense(5, 2))\n",
       "x = rand(10)\n",
       "m(x) == m[2](m[1](x))\n",
       "```\n",
       "\n",
       "`Chain` also supports indexing and slicing, e.g. `m[2]` or `m[1:end-1]`. `m[1:3](x)` will calculate the output of the first three layers.\n"
      ],
      "text/plain": [
       "\u001b[36m  Chain(layers...)\u001b[39m\n",
       "\n",
       "  Chain multiple layers / functions together, so that they are called in\n",
       "  sequence on a given input.\n",
       "\n",
       "\u001b[36m  m = Chain(x -> x^2, x -> x+1)\u001b[39m\n",
       "\u001b[36m  m(5) == 26\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  m = Chain(Dense(10, 5), Dense(5, 2))\u001b[39m\n",
       "\u001b[36m  x = rand(10)\u001b[39m\n",
       "\u001b[36m  m(x) == m[2](m[1](x))\u001b[39m\n",
       "\n",
       "  \u001b[36mChain\u001b[39m also supports indexing and slicing, e.g. \u001b[36mm[2]\u001b[39m or \u001b[36mm[1:end-1]\u001b[39m. \u001b[36mm[1:3](x)\u001b[39m\n",
       "  will calculate the output of the first three layers."
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mArray \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mVector \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mMatrix \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mVecOrMat \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mConvDims\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Dense(in::Integer, out::Integer, σ = identity)\n",
       "\\end{verbatim}\n",
       "Creates a traditional \\texttt{Dense} layer with parameters \\texttt{W} and \\texttt{b}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "y = σ.(W * x .+ b)\n",
       "\\end{verbatim}\n",
       "The input \\texttt{x} must be a vector of length \\texttt{in}, or a batch of vectors represented as an \\texttt{in × N} matrix. The out \\texttt{y} will be a vector or batch of length \\texttt{out}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> d = Dense(5, 2)\n",
       "Dense(5, 2)\n",
       "\n",
       "julia> d(rand(5))\n",
       "Tracked 2-element Array{Float64,1}:\n",
       "  0.00257447\n",
       "  -0.00449443\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "Dense(in::Integer, out::Integer, σ = identity)\n",
       "```\n",
       "\n",
       "Creates a traditional `Dense` layer with parameters `W` and `b`.\n",
       "\n",
       "```\n",
       "y = σ.(W * x .+ b)\n",
       "```\n",
       "\n",
       "The input `x` must be a vector of length `in`, or a batch of vectors represented as an `in × N` matrix. The out `y` will be a vector or batch of length `out`.\n",
       "\n",
       "```julia\n",
       "julia> d = Dense(5, 2)\n",
       "Dense(5, 2)\n",
       "\n",
       "julia> d(rand(5))\n",
       "Tracked 2-element Array{Float64,1}:\n",
       "  0.00257447\n",
       "  -0.00449443\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  Dense(in::Integer, out::Integer, σ = identity)\u001b[39m\n",
       "\n",
       "  Creates a traditional \u001b[36mDense\u001b[39m layer with parameters \u001b[36mW\u001b[39m and \u001b[36mb\u001b[39m.\n",
       "\n",
       "\u001b[36m  y = σ.(W * x .+ b)\u001b[39m\n",
       "\n",
       "  The input \u001b[36mx\u001b[39m must be a vector of length \u001b[36min\u001b[39m, or a batch of vectors represented\n",
       "  as an \u001b[36min × N\u001b[39m matrix. The out \u001b[36my\u001b[39m will be a vector or batch of length \u001b[36mout\u001b[39m.\n",
       "\n",
       "\u001b[36m  julia> d = Dense(5, 2)\u001b[39m\n",
       "\u001b[36m  Dense(5, 2)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> d(rand(5))\u001b[39m\n",
       "\u001b[36m  Tracked 2-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m    0.00257447\u001b[39m\n",
       "\u001b[36m    -0.00449443\u001b[39m"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×4 Array{Float32,2}:\n",
       "  0.775586  -0.304971   0.421589   -0.436464\n",
       " -0.323282  -0.164366   0.0802988  -0.390513\n",
       "  0.347033  -0.63944   -0.0454182  -0.369854"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dense layer is just a matrix multiply; it has param W with shape (NxM); input is M, output is N\n",
    "# This is 4 inputs to three neurons; there is a bias which is an integer\n",
    "# Note the .+ to indicate broadcasting\n",
    "x = [5, 1, 4, 6]\n",
    "b = 0.25\n",
    "W = Dense(4,3).W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1my\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "identity(x)\n",
       "\\end{verbatim}\n",
       "The identity function. Returns its argument.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> identity(\"Well, what did you expect?\")\n",
       "\"Well, what did you expect?\"\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "identity(x)\n",
       "```\n",
       "\n",
       "The identity function. Returns its argument.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> identity(\"Well, what did you expect?\")\n",
       "\"Well, what did you expect?\"\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  identity(x)\u001b[39m\n",
       "\n",
       "  The identity function. Returns its argument.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> identity(\"Well, what did you expect?\")\u001b[39m\n",
       "\u001b[36m  \"Well, what did you expect?\"\u001b[39m"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 4.041679382324219 \n",
       " 0.5067691802978516\n",
       " 8.800432205200195 "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W*x .+ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(4, 3), softmax)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare model taking 4 features as inputs and outputting 3 probabiltiies, \n",
    "# one for each species of iris.\n",
    "model = Chain(\n",
    "    Dense(4, 3),\n",
    "    softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dense(4, 3), NNlib.softmax)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-1.4777225 1.6287514 -2.0652995 -2.1090415; 0.5130562 -0.70725 -0.4703287 -0.9004947; 0.3975363 -0.8862645 2.6626518 2.6652656], Float32[-0.34812948, 2.2498853, -1.901755]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the parameters of the first layer\n",
    "params(model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain means a function that applies step wise functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m! ∇\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m ∇\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m! log\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m log\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m! ∇log\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mx\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "softmax(xs) = exp.(xs) ./ sum(exp.(xs))\n",
       "\\end{verbatim}\n",
       "\\href{https://en.wikipedia.org/wiki/Softmax_function}{Softmax} takes log-probabilities (any real vector) and returns a probability distribution that sums to 1.\n",
       "\n",
       "If given a matrix it will treat it as a batch of vectors, with each column independent.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> softmax([1,2,3.])\n",
       "3-element Array{Float64,1}:\n",
       "  0.0900306\n",
       "  0.244728\n",
       "  0.665241\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "softmax(xs) = exp.(xs) ./ sum(exp.(xs))\n",
       "```\n",
       "\n",
       "[Softmax](https://en.wikipedia.org/wiki/Softmax_function) takes log-probabilities (any real vector) and returns a probability distribution that sums to 1.\n",
       "\n",
       "If given a matrix it will treat it as a batch of vectors, with each column independent.\n",
       "\n",
       "```\n",
       "julia> softmax([1,2,3.])\n",
       "3-element Array{Float64,1}:\n",
       "  0.0900306\n",
       "  0.244728\n",
       "  0.665241\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  softmax(xs) = exp.(xs) ./ sum(exp.(xs))\u001b[39m\n",
       "\n",
       "  Softmax (https://en.wikipedia.org/wiki/Softmax_function) takes\n",
       "  log-probabilities (any real vector) and returns a probability distribution\n",
       "  that sums to 1.\n",
       "\n",
       "  If given a matrix it will treat it as a batch of vectors, with each column\n",
       "  independent.\n",
       "\n",
       "\u001b[36m  julia> softmax([1,2,3.])\u001b[39m\n",
       "\u001b[36m  3-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m    0.0900306\u001b[39m\n",
       "\u001b[36m    0.244728\u001b[39m\n",
       "\u001b[36m    0.665241\u001b[39m"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Descent(0.5)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x, y) = crossentropy(model(x), y)\n",
    "\n",
    "# Gradient descent optimiser with learning rate 0.5.\n",
    "optimiser = Descent(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Descent(η)\n",
       "\n",
       "Classic gradient descent optimiser with learning rate \\texttt{η}. For each parameter \\texttt{p} and its gradient \\texttt{δp}, this runs \\texttt{p -= η*δp}\n",
       "\n",
       "\\subsection{Parameters}\n",
       "\\begin{itemize}\n",
       "\\item Learning Rate (η): The amount by which the gradients are discounted before updating the weights. Defaults to \\texttt{0.1}.\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Example}\n",
       "\\begin{verbatim}\n",
       "opt = Descent() # uses default η (0.1)\n",
       "\n",
       "opt = Descent(0.3) # use provided η\n",
       "\n",
       "ps = params(model)\n",
       "\n",
       "gs = gradient(ps) do\n",
       "  loss(x, y)\n",
       "end\n",
       "\n",
       "Flux.Optimise.update!(opt, ps, gs)\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "Descent(η)\n",
       "\n",
       "Classic gradient descent optimiser with learning rate `η`. For each parameter `p` and its gradient `δp`, this runs `p -= η*δp`\n",
       "\n",
       "## Parameters\n",
       "\n",
       "  * Learning Rate (η): The amount by which the gradients are discounted before updating the weights. Defaults to `0.1`.\n",
       "\n",
       "## Example\n",
       "\n",
       "```julia-repl\n",
       "opt = Descent() # uses default η (0.1)\n",
       "\n",
       "opt = Descent(0.3) # use provided η\n",
       "\n",
       "ps = params(model)\n",
       "\n",
       "gs = gradient(ps) do\n",
       "  loss(x, y)\n",
       "end\n",
       "\n",
       "Flux.Optimise.update!(opt, ps, gs)\n",
       "```\n"
      ],
      "text/plain": [
       "  Descent(η)\n",
       "\n",
       "  Classic gradient descent optimiser with learning rate \u001b[36mη\u001b[39m. For each parameter\n",
       "  \u001b[36mp\u001b[39m and its gradient \u001b[36mδp\u001b[39m, this runs \u001b[36mp -= η*δp\u001b[39m\n",
       "\n",
       "\u001b[1m  Parameters\u001b[22m\n",
       "\u001b[1m  ============\u001b[22m\n",
       "\n",
       "    •    Learning Rate (η): The amount by which the gradients are\n",
       "        discounted before updating the weights. Defaults to \u001b[36m0.1\u001b[39m.\n",
       "\n",
       "\u001b[1m  Example\u001b[22m\n",
       "\u001b[1m  =========\u001b[22m\n",
       "\n",
       "\u001b[36m  opt = Descent() # uses default η (0.1)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  opt = Descent(0.3) # use provided η\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  ps = params(model)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  gs = gradient(ps) do\u001b[39m\n",
       "\u001b[36m    loss(x, y)\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  Flux.Optimise.update!(opt, ps, gs)\u001b[39m"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Iterators.Take{Base.Iterators.Repeated{Tuple{Array{Float64,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}}}(Base.Iterators.Repeated{Tuple{Array{Float64,2},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}}(([0.5533332750260047 -0.9006811702978109 … -0.05250607719225172 -0.5371775589668563; -1.281972426831577 1.0320572244889543 … -0.5877635314354182 0.800654259356901; 0.705892938853251 -1.3412724047598341 … 0.7627586428425032 -1.2844067007705817; 0.9220637630692059 -1.3129767272601456 … 1.579428613166074 -1.0500307872213983], Bool[0 1 … 0 1; 0 0 … 0 0; 1 0 … 1 0])), 150)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create iterator to train model over 110 epochs.\n",
    "data_iterator = Iterators.repeated((X_train, y_train), 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "repeated(x[, n::Int])\n",
       "\\end{verbatim}\n",
       "An iterator that generates the value \\texttt{x} forever. If \\texttt{n} is specified, generates \\texttt{x} that many times (equivalent to \\texttt{take(repeated(x), n)}).\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> a = Iterators.repeated([1 2], 4);\n",
       "\n",
       "julia> collect(a)\n",
       "4-element Array{Array{Int64,2},1}:\n",
       " [1 2]\n",
       " [1 2]\n",
       " [1 2]\n",
       " [1 2]\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "repeated(x[, n::Int])\n",
       "```\n",
       "\n",
       "An iterator that generates the value `x` forever. If `n` is specified, generates `x` that many times (equivalent to `take(repeated(x), n)`).\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> a = Iterators.repeated([1 2], 4);\n",
       "\n",
       "julia> collect(a)\n",
       "4-element Array{Array{Int64,2},1}:\n",
       " [1 2]\n",
       " [1 2]\n",
       " [1 2]\n",
       " [1 2]\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  repeated(x[, n::Int])\u001b[39m\n",
       "\n",
       "  An iterator that generates the value \u001b[36mx\u001b[39m forever. If \u001b[36mn\u001b[39m is specified, generates\n",
       "  \u001b[36mx\u001b[39m that many times (equivalent to \u001b[36mtake(repeated(x), n)\u001b[39m).\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> a = Iterators.repeated([1 2], 4);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> collect(a)\u001b[39m\n",
       "\u001b[36m  4-element Array{Array{Int64,2},1}:\u001b[39m\n",
       "\u001b[36m   [1 2]\u001b[39m\n",
       "\u001b[36m   [1 2]\u001b[39m\n",
       "\u001b[36m   [1 2]\u001b[39m\n",
       "\u001b[36m   [1 2]\u001b[39m"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Iterators.repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "train!(loss, params, data, opt; cb)\n",
       "\\end{verbatim}\n",
       "For each datapoint \\texttt{d} in \\texttt{data} computes the gradient of \\texttt{loss(d...)} through backpropagation and calls the optimizer \\texttt{opt}.\n",
       "\n",
       "Takes a callback as keyword argument \\texttt{cb}. For example, this will print \"training\" every 10 seconds:\n",
       "\n",
       "\\begin{verbatim}\n",
       "Flux.train!(loss, params, data, opt,\n",
       "            cb = throttle(() -> println(\"training\"), 10))\n",
       "\\end{verbatim}\n",
       "The callback can call \\texttt{Flux.stop()} to interrupt the training loop.\n",
       "\n",
       "Multiple optimisers and callbacks can be passed to \\texttt{opt} and \\texttt{cb} as arrays.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "train!(loss, params, data, opt; cb)\n",
       "```\n",
       "\n",
       "For each datapoint `d` in `data` computes the gradient of `loss(d...)` through backpropagation and calls the optimizer `opt`.\n",
       "\n",
       "Takes a callback as keyword argument `cb`. For example, this will print \"training\" every 10 seconds:\n",
       "\n",
       "```julia\n",
       "Flux.train!(loss, params, data, opt,\n",
       "            cb = throttle(() -> println(\"training\"), 10))\n",
       "```\n",
       "\n",
       "The callback can call `Flux.stop()` to interrupt the training loop.\n",
       "\n",
       "Multiple optimisers and callbacks can be passed to `opt` and `cb` as arrays.\n"
      ],
      "text/plain": [
       "\u001b[36m  train!(loss, params, data, opt; cb)\u001b[39m\n",
       "\n",
       "  For each datapoint \u001b[36md\u001b[39m in \u001b[36mdata\u001b[39m computes the gradient of \u001b[36mloss(d...)\u001b[39m through\n",
       "  backpropagation and calls the optimizer \u001b[36mopt\u001b[39m.\n",
       "\n",
       "  Takes a callback as keyword argument \u001b[36mcb\u001b[39m. For example, this will print\n",
       "  \"training\" every 10 seconds:\n",
       "\n",
       "\u001b[36m  Flux.train!(loss, params, data, opt,\u001b[39m\n",
       "\u001b[36m              cb = throttle(() -> println(\"training\"), 10))\u001b[39m\n",
       "\n",
       "  The callback can call \u001b[36mFlux.stop()\u001b[39m to interrupt the training loop.\n",
       "\n",
       "  Multiple optimisers and callbacks can be passed to \u001b[36mopt\u001b[39m and \u001b[36mcb\u001b[39m as arrays."
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Flux.train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.026197 seconds (458.13 k allocations: 11.549 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training done...\n",
      "└ @ Main In[259]:2\n"
     ]
    }
   ],
   "source": [
    "@time Flux.train!(loss, params(model), data_iterator, optimiser)\n",
    "@info \"Training done...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: SkipC\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22mn\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22mti\u001b[0m\u001b[1mo\u001b[22mn Exp\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22mntialBa\u001b[0m\u001b[1mc\u001b[22mk\u001b[0m\u001b[1mO\u001b[22mff\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "onecold(y[, labels = 1:length(y)])\n",
       "\\end{verbatim}\n",
       "Inverse operations of \\href{@ref}{\\texttt{onehot}}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> using Flux: onecold\n",
       "\n",
       "julia> onecold([true, false, false], [:a, :b, :c])\n",
       ":a\n",
       "\n",
       "julia> onecold([0.3, 0.2, 0.5], [:a, :b, :c])\n",
       ":c\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "onecold(y[, labels = 1:length(y)])\n",
       "```\n",
       "\n",
       "Inverse operations of [`onehot`](@ref).\n",
       "\n",
       "```jldoctest\n",
       "julia> using Flux: onecold\n",
       "\n",
       "julia> onecold([true, false, false], [:a, :b, :c])\n",
       ":a\n",
       "\n",
       "julia> onecold([0.3, 0.2, 0.5], [:a, :b, :c])\n",
       ":c\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  onecold(y[, labels = 1:length(y)])\u001b[39m\n",
       "\n",
       "  Inverse operations of \u001b[36monehot\u001b[39m.\n",
       "\n",
       "\u001b[36m  julia> using Flux: onecold\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> onecold([true, false, false], [:a, :b, :c])\u001b[39m\n",
       "\u001b[36m  :a\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> onecold([0.3, 0.2, 0.5], [:a, :b, :c])\u001b[39m\n",
       "\u001b[36m  :c\u001b[39m"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?onecold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 2\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " ⋮\n",
       " 1\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 3"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#onecold is like argmax\n",
    "onecold(model(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×52 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
       " 0  0  0  0  0  1  0  1  1  1  1  1  0  …  1  0  0  0  0  0  1  1  0  0  0  0\n",
       " 1  0  0  1  0  0  1  0  0  0  0  0  1     0  0  0  0  0  1  0  0  0  0  0  0\n",
       " 0  1  1  0  1  0  0  0  0  0  0  0  0     0  1  1  1  1  0  0  0  1  1  1  1"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate trained model against test set.\n",
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.005635 seconds (2.87 k allocations: 138.418 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time accuracy_score = accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confusion_matrix (generic function with 1 method)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function confusion_matrix(X, y)\n",
    "    ŷ = onehotbatch(onecold(model(X)), 1:3)\n",
    "    y * ŷ'\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Int64,2}:\n",
       " 16   0   0\n",
       "  0  15   2\n",
       "  0   2  17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(confusion_matrix(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
