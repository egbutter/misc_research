{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `/opt/julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m   Cloning\u001b[22m\u001b[39m git-repo `https://github.com/JuliaComputing/JuliaAcademyData.jl`\n",
      "\u001b[2K\u001b[?25h\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaComputing/JuliaAcademyData.jl`ng:\u001b[22m\u001b[39m [=====================>                   ]  51.5 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [========================>                ]  59.2 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [===========================>             ]  67.0 %\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaComputing/JuliaAcademyData.jl`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `/opt/julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [18b7da76]\u001b[39m\u001b[92m + JuliaAcademyData v0.1.0 #master (https://github.com/JuliaComputing/JuliaAcademyData.jl)\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `/opt/julia/environments/v1.2/Manifest.toml`\n",
      " \u001b[90m [18b7da76]\u001b[39m\u001b[92m + JuliaAcademyData v0.1.0 #master (https://github.com/JuliaComputing/JuliaAcademyData.jl)\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling JuliaAcademyData [18b7da76-0988-5e3b-acac-6290be3a708f]\n",
      "└ @ Base loading.jl:1242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mActivating\u001b[22m\u001b[39m environment at `/opt/julia/packages/JuliaAcademyData/II8LR/courses/Deep learning with Flux/Project.toml`\n"
     ]
    },
    {
     "ename": "Pkg.Types.PkgError",
     "evalue": "Package CredentialsHandler [864e158e-919d-11e8-198e-cfe890ec4681] not found in a registry.",
     "output_type": "error",
     "traceback": [
      "Package CredentialsHandler [864e158e-919d-11e8-198e-cfe890ec4681] not found in a registry.",
      "",
      "Stacktrace:",
      " [1] pkgerror(::String) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Pkg/src/Types.jl:112",
      " [2] check_registered(::Pkg.Types.Context, ::Array{Pkg.Types.PackageSpec,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Pkg/src/Operations.jl:924",
      " [3] #instantiate#81(::Nothing, ::Bool, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(Pkg.API.instantiate), ::Pkg.Types.Context) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Pkg/src/API.jl:480",
      " [4] instantiate at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Pkg/src/API.jl:461 [inlined]",
      " [5] #instantiate#80 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Pkg/src/API.jl:458 [inlined]",
      " [6] instantiate at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Pkg/src/API.jl:458 [inlined]",
      " [7] activate(::String) at /opt/julia/packages/JuliaAcademyData/II8LR/src/JuliaAcademyData.jl:30",
      " [8] top-level scope at In[1]:2"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(Pkg.PackageSpec(url=\"https://github.com/JuliaComputing/JuliaAcademyData.jl\"))\n",
    "using JuliaAcademyData; activate(\"Deep learning with Flux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "# Intro to Flux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous course, we learned how machine learning allows us to classify data as apples or bananas with a single neuron. However, some of those details are pretty fiddly! Fortunately, Julia has a powerful package that does much of the heavy lifting for us, called [`Flux.jl`](https://fluxml.github.io/).\n",
    "\n",
    "*Using `Flux` will make classifying data and images much easier!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `Flux.jl`\n",
    "\n",
    "We can get started with `Flux.jl` via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /opt/julia/compiled/v1.2/Flux/QdkVy.ji for Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
      "└ @ Base loading.jl:1240\n",
      "ERROR: LoadError: ArgumentError: Package Flux does not have Zygote in its dependencies:\n",
      "- If you have Flux checked out for development and have\n",
      "  added Zygote as a dependency but haven't updated your primary\n",
      "  environment's manifest file, try `Pkg.resolve()`.\n",
      "- Otherwise you may need to report an issue with Flux\n",
      "Stacktrace:\n",
      " [1] require(::Module, ::Symbol) at ./loading.jl:889\n",
      " [2] include at ./boot.jl:328 [inlined]\n",
      " [3] include_relative(::Module, ::String) at ./loading.jl:1094\n",
      " [4] include(::Module, ::String) at ./Base.jl:31\n",
      " [5] top-level scope at none:2\n",
      " [6] eval at ./boot.jl:330 [inlined]\n",
      " [7] eval(::Expr) at ./client.jl:432\n",
      " [8] top-level scope at ./none:3\n",
      "in expression starting at /opt/julia/packages/Flux/oX9Pi/src/Flux.jl:6\n"
     ]
    },
    {
     "ename": "ErrorException",
     "evalue": "Failed to precompile Flux [587475ba-b771-5e3f-ad9e-33799f191a9c] to /opt/julia/compiled/v1.2/Flux/QdkVy.ji.",
     "output_type": "error",
     "traceback": [
      "Failed to precompile Flux [587475ba-b771-5e3f-ad9e-33799f191a9c] to /opt/julia/compiled/v1.2/Flux/QdkVy.ji.",
      "",
      "Stacktrace:",
      " [1] error(::String) at ./error.jl:33",
      " [2] compilecache(::Base.PkgId, ::String) at ./loading.jl:1253",
      " [3] _require(::Base.PkgId) at ./loading.jl:1013",
      " [4] require(::Base.PkgId) at ./loading.jl:911",
      " [5] require(::Module, ::Symbol) at ./loading.jl:906",
      " [6] top-level scope at In[2]:1"
     ]
    }
   ],
   "source": [
    "# using Pkg; Pkg.add([\"Flux\", \"Plots\"])\n",
    "using Flux, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful built-in functions\n",
    "\n",
    "When working we'll `Flux`, we'll make use of built-in functionality that we've had to create for ourselves in previous notebooks.\n",
    "\n",
    "For example, the sigmoid function, σ, that we have been using already lives within `Flux`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(σ, -5, 5, label=\"\\\\sigma\", xlabel=\"x\", ylabel=\"\\\\sigma\\\\(x\\\\)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, `Flux` allows us to *automatically create neurons* with the **`Dense`** function. For example, in the last notebook, we were looking at a neuron with 2 inputs and 1 output:\n",
    "\n",
    " <img src=\"https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Deep%20learning%20with%20Flux/data/single-neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    " We could create a neuron with two inputs and one output via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(2, 1, σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `model` object comes with places to store weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(model.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(2)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ.(model.W*x + model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in previous notebooks, note that `W` is no longer a `Vector` (1D `Array`) and `b` is no longer a number! Both are now stored in so-called `TrackedArray`s and `W` is effectively being treated as a matrix with a single row. We'll see why below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other helpful built-in functionality includes ways to automatically calculate gradients and also the cost function that we've used in the previous course -\n",
    "\n",
    "$$L(w, b) = \\sum_i \\left[y_i - f(x_i, w, b) \\right]^2$$\n",
    "\n",
    "If you normalize by dividing by the total number of elements, this becomes the \"mean square error\" function, which in `Flux` is named **`Flux.mse`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods(Flux.mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "Load the datasets that contain the features of the apple and banana images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames\n",
    "\n",
    "apples = DataFrame(CSV.File(datapath(\"data/apples.dat\"), delim='\\t', allowmissing=:none, normalizenames=true))\n",
    "bananas = DataFrame(CSV.File(datapath(\"data/bananas.dat\"), delim='\\t', allowmissing=:none, normalizenames=true));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_apples  = [ [row.red, row.green] for row in eachrow(apples)]\n",
    "x_bananas = [ [row.red, row.green] for row in eachrow(bananas)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the x (features) together to create a vector of all our datapoints, and create the corresponding vector of known labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [x_apples; x_bananas]\n",
    "ys = [fill(0, size(x_apples)); fill(1, size(x_bananas))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(2, 1, σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the model (currently initialized with random weights) to see what the output value is for a given input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(xs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we can examine the current loss value for that datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Flux.mse(model(xs[1]), ys[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Tracker\n",
    "back!(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the tools necessary to build a simple gradient descent algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The easy way\n",
    "\n",
    "You don't want to manually write out gradient descent algorithms every time! Flux, of course, also brings in lots of optimizers that can do this all for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Flux.train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can simply define our loss function, an optimizer, and then call `train!`. That's basic machine learning with Flux.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dense(2, 1, σ)\n",
    "L(x,y) = Flux.mse(model(x), y)\n",
    "opt = SGD(params(model))\n",
    "Flux.train!(L, zip(xs, ys), opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(0:.1:1, 0:.1:1, (x, y) -> model([x,y])[].data, fill=true)\n",
    "scatter!(first.(x_apples), last.(x_apples), label=\"apples\")\n",
    "scatter!(first.(x_bananas), last.(x_bananas), label=\"bananas\")\n",
    "xlabel!(\"mean red value\")\n",
    "ylabel!(\"mean green value\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
